import sounddevice as sd
from scipy.io.wavfile import write
import librosa
import numpy as np
import json
from sklearn.metrics.pairwise import cosine_similarity

# === GHI Ã‚M ===
def record_audio(filename="recorded.wav", duration=10, fs=44100):
    print("ðŸŽ™ï¸ Äang ghi Ã¢m...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    write(filename, fs, audio)
    print(f"âœ… Ghi xong. LÆ°u táº¡i {filename}")

# === TRÃCH MFCC + TEMPO + CENTROID ===
def extract_features(file_path):
    """Extract MFCC, tempo, and spectral centroid from an audio file."""
    print("extract_features of", file_path)
    audio, sr = librosa.load(file_path, sr=None)

    # TÃ­nh MFCC
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    # mfcc_mean = mfcc.mean(axis=1).toList() # Normal MFCC
    mfcc_mean = extract_mfcc_pro(file_path).tolist() # Pro MFCC
    # mfcc_mean = extract_mfcc_ultra(file_path).tolist() # Ultra MFCC

    # Tempo (rhythm)
    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)
    tempo = float(tempo) if isinstance(tempo, np.ndarray) else tempo

    # Spectral Centroid (brightness/tone)
    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
    centroid_mean = np.mean(spectral_centroid)

    return mfcc_mean, tempo, centroid_mean


# === Táº¢I DATABASE ===
def load_database(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_mfcc_pro(file_path,
                     sr=44100,
                     n_mfcc=40,
                     n_fft=4096,
                     hop_length=256,
                     use_delta=True):
    """
    TrÃ­ch xuáº¥t MFCC chuyÃªn sÃ¢u:
    - Chuáº©n hÃ³a Ã¢m lÆ°á»£ng
    - Cáº¯t khoáº£ng láº·ng
    - Lá»c pre-emphasis
    - MFCC + delta (biáº¿n Ä‘á»™ng)
    """

    # Load audio vá»›i sampling rate cao
    y, sr = librosa.load(file_path, sr=sr)

    # Normalize biÃªn Ä‘á»™
    y = librosa.util.normalize(y)

    # Cáº¯t khoáº£ng láº·ng (silence)
    y, _ = librosa.effects.trim(y)

    # Pre-emphasis filter (lÃ m ná»•i báº­t táº§n sá»‘ cao)
    pre_emphasis = 0.97
    y_preemph = np.append(y[0], y[1:] - pre_emphasis * y[:-1])

    # TÃ­nh MFCC
    mfcc = librosa.feature.mfcc(
        y=y_preemph,
        sr=sr,
        n_mfcc=n_mfcc,
        n_fft=n_fft,
        hop_length=hop_length
    )

    if use_delta:
        # TÃ­nh delta vÃ  delta-delta
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)
        mfcc_all = np.vstack([mfcc, delta, delta2])
    else:
        mfcc_all = mfcc

    # Tráº£ vá» vector trung bÃ¬nh
    mfcc_mean = mfcc_all.mean(axis=1)

    return mfcc_mean


# TÃ­nh MFCC siÃªu cáº¥p (ultra) vá»›i Ä‘á»™ chÃ­nh xÃ¡c tá»‘i Ä‘a
def extract_mfcc_ultra(file_path,
                       sr=48000,
                       n_mfcc=40,
                       n_fft=4096,
                       hop_length=256,
                       use_delta=True,
                       apply_cmvn=False):
    """
    TrÃ­ch xuáº¥t MFCC siÃªu cáº¥p: Ä‘á»™ chÃ­nh xÃ¡c tá»‘i Ä‘a
    """

    # Load & normalize
    y, _ = librosa.load(file_path, sr=sr)
    y = librosa.util.normalize(y)

    # Trim silence (chÃ­nh xÃ¡c hÆ¡n)
    y, _ = librosa.effects.trim(y, top_db=30)

    # Pre-emphasis filter
    pre_emphasis = 0.97
    y_preemph = np.append(y[0], y[1:] - pre_emphasis * y[:-1])

    # MFCC
    mfcc = librosa.feature.mfcc(
        y=y_preemph,
        sr=sr,
        n_mfcc=n_mfcc,
        n_fft=n_fft,
        hop_length=hop_length
    )

    # ThÃªm log-energy (dÃ²ng Ä‘áº§u tiÃªn lÃ  tá»•ng nÄƒng lÆ°á»£ng khung)
    log_energy = librosa.feature.rms(y=y_preemph, hop_length=hop_length).flatten()
    log_energy = np.log1p(log_energy)  # log(1 + E)
    log_energy = np.expand_dims(log_energy, axis=0)
    mfcc = np.vstack([log_energy, mfcc])  # giá» lÃ  (n_mfcc+1, T)

    if use_delta:
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)
        mfcc = np.vstack([mfcc, delta, delta2])  # (n_feat*3, T)

    if apply_cmvn:
        # Apply Cepstral Mean Variance Normalization
        mfcc = (mfcc - np.mean(mfcc, axis=1, keepdims=True)) / \
               (np.std(mfcc, axis=1, keepdims=True) + 1e-6)

    mfcc_mean = mfcc.mean(axis=1)
    return mfcc_mean




# === SO KHá»šP MFCC + TEMPO + CENTROID ===
# ðŸŽ¯ Nháº­n diá»‡n cá»±c chÃ­nh xÃ¡c (báº£n gá»‘c 100%)	MFCC â‰¥ 0.95, tempo â‰¤ 5, centroid â‰¤ 150
# ðŸŽµ Nháº­n diá»‡n báº£n phá»‘i láº¡i	                MFCC â‰¥ 0.85, tempo â‰¤ 20, centroid â‰¤ 500
# ðŸŽ™ï¸ Nháº­n diá»‡n ngÆ°á»i huÃ½t sÃ¡o/hÃ¡t láº¡i	    MFCC â‰¥ 0.75, tempo â‰¤ 25, centroid bá» qua
def find_best_match_whistle(query_vec, query_tempo, query_centroid, database,
                            mfcc_weight=0.65, tempo_weight=0.25, centroid_weight=0.1):
    candidates = []

    for song in database:
        db_vec = np.array(song["mfcc"])


        sim_mfcc = np.dot(query_vec, song['mfcc']) / (
                np.linalg.norm(query_vec) * np.linalg.norm(song['mfcc'])
        )

        # sim_mfcc = np.dot(query_vec, db_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(db_vec))
        # sim_mfcc = cosine_similarity([query_vec], [db_vec])[0][0]

        tempo_diff = abs(query_tempo - song["tempo"])
        centroid_diff = abs(query_centroid - song["spectral_centroid"])

        # Debug output
        print(f"\nðŸŽµ {song['title']} - {song['artist_name']}")
        print(f"MFCC: {sim_mfcc:.3f}, Tempo Diff: {tempo_diff:.2f} BPM, Centroid Diff: {centroid_diff:.2f} Hz")

        # Sá»­ dá»¥ng threshold gáº§n sÃ¡t 100%
        # if sim_mfcc >= 0.98 or tempo_diff <= 2 or centroid_diff <= 50:
        #     candidates.append((song, 0.98)
        #     continue

        # Lá»c theo threshold
        if sim_mfcc >= 0.75 and tempo_diff <= 25 and centroid_diff <= 500:
            # Chuáº©n hoÃ¡ cÃ¡c diff vá» [0â€“1] rá»“i chuyá»ƒn thÃ nh Ä‘iá»ƒm
            tempo_score = 1 - (tempo_diff / 25)
            centroid_score = 1 - (centroid_diff / 500)

            score = sim_mfcc * mfcc_weight + tempo_score * tempo_weight + centroid_score * centroid_weight
            candidates.append((song, score))

    # Tráº£ vá» bÃ i cÃ³ tá»•ng Ä‘iá»ƒm cao nháº¥t
    if candidates:
        candidates.sort(key=lambda x: x[1], reverse=True)
        print("\nðŸ” CÃ¡c bÃ i hÃ¡t kháº£ thi:")
        for song, score in candidates:
            print(f"  - {song['title']} - {song['artist_name']}: {score:.3f}")

        best_song = candidates[0][0]
        print(f"\nâœ… Nháº­n diá»‡n: {best_song['title']} - {best_song['artist_name']}")
        return best_song
    else:
        print("\nâŒ KhÃ´ng cÃ³ bÃ i nÃ o Ä‘áº¡t Ä‘á»§ Ä‘iá»u kiá»‡n threshold.")
        return None


# === CHáº Y TOÃ€N Bá»˜ ===
if __name__ == "__main__":
    record_audio()
    mfcc, tempo, centroid = extract_features("recorded.wav")
    db = load_database("fma_20_songs_db.json")
    find_best_match_whistle(mfcc, tempo, centroid, db)

